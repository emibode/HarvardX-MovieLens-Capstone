---
title: "MovieLens Capstone Project"
author: "Emi Bode"
date: "2024/05/06"
output:
  pdf_document: default
  word_document: default
---

Introduction 

To develop a recommendation system for movies based on user ratings, we'll create an algorithm that predicts the preferences or ratings users would assign to different movies. This system will analyze ratings provided by users to generate personalized movie suggestions. Just like Netflix, we aim to forecast how users might rate specific movies. Inspired by successful strategies from previous Netflix challenges, we're embarking on a similar endeavor. In October 2006, Netflix initiated a challenge to enhance their recommendation algorithm, offering a million-dollar prize for a 10% improvement. By September 2009, the winners had been announced. For insights into how the winning algorithm was crafted, you can explore a summary and detailed explanation provided here. Now, we'll delve into the data analysis strategies utilized by the winning team to create our own movie recommendation system.

The aim of this assignment is to develop a recommendation system focused on suggesting movies, leveraging a rating scale as a key component.

Dataset 

For this project, we'll be utilizing the MovieLens dataset compiled by GroupLens Research, which is available on the MovieLens website (<http://movielens.org>).

Data Loading 

The dataset is loaded using the code provided by the course instructor, accessible through the following link:
<https://bit.ly/2Ng6tVW>. This code splits the data into two parts: an initial dataset (edx set) and a 10% validation set. The edx set is further divided into training and test sets, while the validation set
remains separate and is reserved for final evaluation purposes.

```{r }
#############################################################
# Create edx set, validation set, and submission file
#############################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Final hold-out test set will be 10% of MovieLens data

set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,] 

# Make sure userId and movieId in validation set are also in edx set validation set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

###################################################################################
#################################################
```

Verifying the data for any NA values

```{r}
anyNA(edx)
```

Data Overview and Initial Analysis

After loading the dataset, our initial step involves examining its structure and data types. We observe six variables: userId, movieID, rating, timestamp, title, and genres. It's noteworthy that for predictive purposes, we may need to separate the year from the title, and similarly, the genres might require separation for further analysis or prediction tasks.

```{r}
str(edx)
summary(edx)
```

From the data summary, we observe that the ratings range from a minimum of 1 to a maximum of 5. The mean rating is calculated at 3.512, with a mode of 4.0, indicating a tendency towards higher ratings.

```{r echo=FALSE}
edx %>% group_by(rating) %>% summarize(count = n()) %>% top_n(5) %>%
	arrange(desc(count))  
```

Obtaining the count of movies and users in the dataset:

```{r echo= FALSE}
edx %>% 
  summarize(n_users = n_distinct(userId),
            n_movies = n_distinct(movieId))

```

Determining the number of ratings received by each movie:

```{r  echo=FALSE}
edx %>% count(movieId) %>% ggplot(aes(n))+
  geom_histogram(color = "black" , fill= "grey",bins = 30 , binwidth = 0.2)+
  scale_x_log10()+
  ggtitle("Number of ratings per movie")+
  theme_gray()
```

We observe variations in the number of ratings received by different movies, possibly indicating differences in popularity. To further explore this, we visualize the distribution of ratings for each user.

```{r echo= FALSE }
edx %>% count(userId) %>% ggplot(aes(n))+
  geom_histogram(color = "black" , fill= "red" , bins = 30, binwidth = 0.2)+
  ggtitle("Number of ratings per user")+
  scale_x_log10()+
  theme_gray()

```

We notice discrepancies in user activity levels, with certain users rating movies more frequently than others.

Calculating the number of ratings for each movie genre:

```{r echo=FALSE }
edx %>% separate_rows(genres, sep = "\\|") %>%
  group_by(genres) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>% ggplot(aes(genres,count)) + 
  geom_bar(aes(fill =genres),stat = "identity")+ 
  labs(title = "Number of ratings for each genre")+
  theme(axis.text.x  = element_text(angle= 90, vjust = 50 ))+
  theme_light()
 
```

Let's explore the top 10 most popular genres.

```{r echo= FALSE }
edx %>% separate_rows(genres, sep = "\\|") %>%
	group_by(genres) %>%
	summarize(count = n()) %>%
	arrange(desc(count))
```

Partitioning the data:

Before constructing the model, we partition the edx dataset, allocating 20% for the test set and reserving 80% for the training set.

```{r}
set.seed(1)
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]
```

RMSE calculation Function

In the Netflix challenge, a standard error loss metric was employed. The winner was determined based on the residual mean squared error (RMSE) calculated on a designated test set. RMSE serves as the measure of accuracy for the evaluation.

```{r}

RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2, na.rm = TRUE))
}
```

The first model:

In our initial model, we adopt a simplistic approach where the same rating is predicted for all movies, irrespective of the user. This model assumes a uniform rating across all movies and users, without considering any biases. The method operates on the assumption of the following linear equation:

$Y~u,i~ = ?? + ??~u,i~$

```{r}
Mu_1 <- mean(train_set$rating)
Mu_1

```

```{r}
naive_rmse <- RMSE(test_set$rating,Mu_1)
naive_rmse
```

This code generates a table to store the RMSE results obtained from each method, facilitating comparison between different approaches.

```{r}
rmse_results <- data_frame(method = "Just the average", RMSE = naive_rmse)
rmse_results%>% knitr::kable()
```

Creating the second model:

As observed during the exploratory analysis certain movies received more ratings compared to others. We can enhance our previous model by incorporating the term b_i to denote the average rating for movie i. We can once again utilize least squares to estimate the movie effect

$Y~u,i~ = ?? + b~i~ + ??~u,i~$ 
Due to the large number of parameters $b~i$ corresponding to each movie, employing the lm() function directly can lead to significant computational slowdown. Therefore, we opt for a more efficient approach by computing it using the average, as follows:

```{r}
Mu_2 <- mean(train_set$rating) 
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - Mu_2))
```

We observe variability in the estimate, as depicted in the plot presented here:

```{r echo=FALSE}
movie_avgs %>% qplot(b_i, geom ="histogram", bins = 10, data = ., color = I("black"))

```

Let's examine how the prediction accuracy improves after modifying the equation $Y~u,i~ = ?? + b~i$

```{r}
predicted_ratings <- Mu_2 + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)

model_2_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie Effect Model",  
                                     RMSE = model_2_rmse))
rmse_results %>% knitr::kable()
                
```

Creating the third model: 

Comparing users who have rated more than 100 movies:

```{r echo= FALSE}
train_set %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black")
```

There is significant variability observed across user ratings as well. This suggests that further improvement to our model may be nessesary such as: $Y~u,i~ = ?? + b~i~ + ??~u,i~$ 
We could fit this model by using use the lm() function but as mentioned earlier, it would be very slow due to large dataset lm(rating \~ as.factor(movieId) + as.factor(userId)) 

Here is the code:

```{r}
user_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - Mu_2 - b_i))
  
```

Now, let's examine how the RMSE has improved this time:

```{r}
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = Mu_2 + b_i + b_u) %>%
  pull(pred)


model_3_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model",  
                                     RMSE = model_3_rmse))
rmse_results%>% knitr::kable()

```

The RMSE of the validation set is:

```{r}
valid_pred_rating <- validation %>%
  left_join(movie_avgs , by = "movieId" ) %>% 
  left_join(user_avgs , by = "userId") %>%
  mutate(pred = Mu_2 + b_i + b_u ) %>%
  pull(pred)

model_3_valid <- RMSE(validation$rating, valid_pred_rating)
rmse_results <-  bind_rows( rmse_results, data_frame(Method = "Validation Results" , RMSE = model_3_valid))
rmse_results%>% knitr::kable()
```

Conclusion

We've developed three strategies: a naive approach, a model focusing on movie-specific effects, and a more complex model integrating both user and movie effects. Among these, the third model yielded the most
promising RMSE results. To delve deeper into analysis, I propose a more intricate prediction strategy leveraging the release year of each movie as a bias. This approach involves categorizing older movies, such as those from the 60s or 80s, as distinct genres, thereby refining our predictive model. For optimal precision, I recommend employing a linear model to accommodate these additional factors.
